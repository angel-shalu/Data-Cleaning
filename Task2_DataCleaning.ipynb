{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7bdaa729",
   "metadata": {},
   "source": [
    "# Task 2: Data Cleaning - TechnoHacks Internship\n",
    "\n",
    "## üéØ Objective:\n",
    "To clean the dataset by checking for:\n",
    "\n",
    "- **Missing values**\n",
    "- **Duplicate rows**\n",
    "- **Invalid entries**\n",
    "- **Consistency in data types**\n",
    "\n",
    "And save the cleaned dataset for further analysis.\n",
    "\n",
    "## üìå Dataset:\n",
    "**Iris Dataset** - Building upon the data collected in Task 1\n",
    "\n",
    "## üß† Step-by-Step Implementation:\n",
    "This notebook will systematically clean the Iris dataset to ensure it's ready for visualization and machine learning tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc448f4f",
   "metadata": {},
   "source": [
    "## üîπ 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a86477ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n",
      "Ready to start data cleaning process...\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "print(\"Libraries imported successfully!\")\n",
    "print(\"Ready to start data cleaning process...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf95666f",
   "metadata": {},
   "source": [
    "## üîπ 2. Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca83e727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded successfully!\n",
      "Original dataset shape: (150, 5)\n",
      "Columns: ['sepal_length', 'sepal_width', 'petal_length', 'petal_width', 'class']\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"iris_dataset.csv\")\n",
    "print(\"Dataset loaded successfully!\")\n",
    "print(f\"Original dataset shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f8d781",
   "metadata": {},
   "source": [
    "## üîπ 3. Initial Data Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d4f3a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 rows of the dataset:\n",
      "   sepal_length  sepal_width  petal_length  petal_width        class\n",
      "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
      "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
      "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
      "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
      "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
      "\n",
      "==================================================\n",
      "Dataset Information:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150 entries, 0 to 149\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   sepal_length  150 non-null    float64\n",
      " 1   sepal_width   150 non-null    float64\n",
      " 2   petal_length  150 non-null    float64\n",
      " 3   petal_width   150 non-null    float64\n",
      " 4   class         150 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 6.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# View first few rows\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Dataset Information:\")\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e7a2c",
   "metadata": {},
   "source": [
    "## üîπ 4. Check for Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "61f75b3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values per column:\n",
      "sepal_length    0\n",
      "sepal_width     0\n",
      "petal_length    0\n",
      "petal_width     0\n",
      "class           0\n",
      "dtype: int64\n",
      "\n",
      "Total missing values: 0\n",
      "‚úÖ No missing values found in this dataset!\n"
     ]
    }
   ],
   "source": [
    "# Check for missing values\n",
    "print(\"Missing values per column:\")\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values)\n",
    "print(f\"\\nTotal missing values: {missing_values.sum()}\")\n",
    "\n",
    "if missing_values.sum() == 0:\n",
    "    print(\"‚úÖ No missing values found in this dataset!\")\n",
    "else:\n",
    "    print(\"‚ùå Missing values detected - need to handle them\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11cc5ab7",
   "metadata": {},
   "source": [
    "## üîπ 5. Identify and Handle Duplicate Rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c2359b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of duplicate rows: 3\n",
      "Shape before removing duplicates: (150, 5)\n",
      "‚úÖ 3 duplicate row(s) removed!\n",
      "Shape after removing duplicates: (147, 5)\n",
      "Final shape: (147, 5)\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate rows\n",
    "duplicates = df.duplicated()\n",
    "print(\"Number of duplicate rows:\", duplicates.sum())\n",
    "print(f\"Shape before removing duplicates: {df.shape}\")\n",
    "\n",
    "# Remove duplicates if any\n",
    "if duplicates.sum() > 0:\n",
    "    df = df.drop_duplicates()\n",
    "    print(f\"‚úÖ {duplicates.sum()} duplicate row(s) removed!\")\n",
    "    print(f\"Shape after removing duplicates: {df.shape}\")\n",
    "else:\n",
    "    print(\"‚úÖ No duplicate rows found!\")\n",
    "\n",
    "# Reset index after removing duplicates\n",
    "df = df.reset_index(drop=True)\n",
    "print(f\"Final shape: {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fecfc2",
   "metadata": {},
   "source": [
    "## üîπ 6. Validate Data Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e707a387",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data types:\n",
      "sepal_length    float64\n",
      "sepal_width     float64\n",
      "petal_length    float64\n",
      "petal_width     float64\n",
      "class            object\n",
      "dtype: object\n",
      "\n",
      "‚úÖ Data Type Validation:\n",
      "Numeric columns (should be float64): ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
      "Categorical columns (should be object): ['class']\n",
      "‚úÖ All data types are correct!\n"
     ]
    }
   ],
   "source": [
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Expected data types validation\n",
    "expected_numeric = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "expected_categorical = ['class']\n",
    "\n",
    "print(f\"\\n‚úÖ Data Type Validation:\")\n",
    "print(f\"Numeric columns (should be float64): {expected_numeric}\")\n",
    "print(f\"Categorical columns (should be object): {expected_categorical}\")\n",
    "\n",
    "# Verify all numeric columns are float64\n",
    "numeric_types_ok = all(df[col].dtype == 'float64' for col in expected_numeric)\n",
    "categorical_types_ok = all(df[col].dtype == 'object' for col in expected_categorical)\n",
    "\n",
    "if numeric_types_ok and categorical_types_ok:\n",
    "    print(\"‚úÖ All data types are correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Some data types need correction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0448b2da",
   "metadata": {},
   "source": [
    "## üîπ 7. Check for Outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bb92ac2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistical Summary:\n",
      "       sepal_length  sepal_width  petal_length  petal_width\n",
      "count    147.000000   147.000000    147.000000   147.000000\n",
      "mean       5.856463     3.055782      3.780272     1.208844\n",
      "std        0.829100     0.437009      1.759111     0.757874\n",
      "min        4.300000     2.000000      1.000000     0.100000\n",
      "25%        5.100000     2.800000      1.600000     0.300000\n",
      "50%        5.800000     3.000000      4.400000     1.300000\n",
      "75%        6.400000     3.300000      5.100000     1.800000\n",
      "max        7.900000     4.400000      6.900000     2.500000\n",
      "\n",
      "üîç Outlier Detection (using IQR method):\n",
      "sepal_length: 0 potential outliers\n",
      "sepal_width: 4 potential outliers\n",
      "petal_length: 0 potential outliers\n",
      "petal_width: 0 potential outliers\n",
      "\n",
      "‚úÖ Note: For the Iris dataset, these are natural variations and not true outliers to remove.\n"
     ]
    }
   ],
   "source": [
    "# Check for outliers using statistical summary\n",
    "print(\"Statistical Summary:\")\n",
    "print(df.describe())\n",
    "\n",
    "# Check for potential outliers using IQR method\n",
    "print(\"\\nüîç Outlier Detection (using IQR method):\")\n",
    "numeric_cols = ['sepal_length', 'sepal_width', 'petal_length', 'petal_width']\n",
    "\n",
    "for col in numeric_cols:\n",
    "    Q1 = df[col].quantile(0.25)\n",
    "    Q3 = df[col].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df[col] < lower_bound) | (df[col] > upper_bound)]\n",
    "    print(f\"{col}: {len(outliers)} potential outliers\")\n",
    "\n",
    "print(\"\\n‚úÖ Note: For the Iris dataset, these are natural variations and not true outliers to remove.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1febbd1",
   "metadata": {},
   "source": [
    "## üîπ 8. Standardize Class Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e0a1c2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current unique class names:\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "\n",
      "Class value counts:\n",
      "class\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     49\n",
      "Iris-setosa        48\n",
      "Name: count, dtype: int64\n",
      "\n",
      "After standardization:\n",
      "Unique class names: ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "Class value counts:\n",
      "class\n",
      "Iris-versicolor    50\n",
      "Iris-virginica     49\n",
      "Iris-setosa        48\n",
      "Name: count, dtype: int64\n",
      "‚úÖ All class names are standardized and correct!\n"
     ]
    }
   ],
   "source": [
    "# Check current class names\n",
    "print(\"Current unique class names:\")\n",
    "print(df['class'].unique())\n",
    "print(f\"\\nClass value counts:\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Standardize class names (remove extra spaces if any)\n",
    "df['class'] = df['class'].str.strip()\n",
    "\n",
    "# Check again after standardization\n",
    "print(f\"\\nAfter standardization:\")\n",
    "print(\"Unique class names:\", df['class'].unique())\n",
    "print(\"Class value counts:\")\n",
    "print(df['class'].value_counts())\n",
    "\n",
    "# Verify expected classes\n",
    "expected_classes = ['Iris-setosa', 'Iris-versicolor', 'Iris-virginica']\n",
    "if set(df['class'].unique()) == set(expected_classes):\n",
    "    print(\"‚úÖ All class names are standardized and correct!\")\n",
    "else:\n",
    "    print(\"‚ùå Some class names may need attention\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8913dda5",
   "metadata": {},
   "source": [
    "## üîπ 9. Save the Cleaned Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b99417e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Dataset saved successfully as 'iris_cleaned.csv'\n",
      "Final cleaned dataset shape: (147, 5)\n",
      "\n",
      "üéØ FINAL CLEANED DATASET SUMMARY:\n",
      "- Total samples: 147\n",
      "- Total features: 4\n",
      "- Classes: 3\n",
      "- Missing values: 0\n",
      "- Duplicate rows: 0\n",
      "- Data types consistent: ‚úÖ\n",
      "- Class names standardized: ‚úÖ\n",
      "\n",
      " Data cleaning process completed successfully!\n",
      "The cleaned dataset is ready for visualization and modeling.\n"
     ]
    }
   ],
   "source": [
    "# Save the cleaned dataset\n",
    "df.to_csv(\"iris_cleaned.csv\", index=False)\n",
    "print(\" Dataset saved successfully as 'iris_cleaned.csv'\")\n",
    "print(f\"Final cleaned dataset shape: {df.shape}\")\n",
    "\n",
    "# Final verification\n",
    "print(\"\\nüéØ FINAL CLEANED DATASET SUMMARY:\")\n",
    "print(f\"- Total samples: {len(df)}\")\n",
    "print(f\"- Total features: {len(df.columns) - 1}\")\n",
    "print(f\"- Classes: {df['class'].nunique()}\")\n",
    "print(f\"- Missing values: {df.isnull().sum().sum()}\")\n",
    "print(f\"- Duplicate rows: {df.duplicated().sum()}\")\n",
    "print(f\"- Data types consistent: ‚úÖ\")\n",
    "print(f\"- Class names standardized: ‚úÖ\")\n",
    "\n",
    "print(\"\\n Data cleaning process completed successfully!\")\n",
    "print(\"The cleaned dataset is ready for visualization and modeling.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8abdee9",
   "metadata": {},
   "source": [
    "## üìÅ Files Created:\n",
    "- `iris_cleaned.csv` ‚Äì Cleaned and ready-to-use dataset\n",
    "- `Task2_DataCleaning.ipynb` ‚Äì This notebook with all cleaning steps\n",
    "\n",
    "## üìå Summary:\n",
    "‚úÖ **Task 2: Data Cleaning Completed Successfully!**\n",
    "\n",
    "In this task, we:\n",
    "1. ‚úÖ **Imported required libraries** (pandas, numpy)\n",
    "2. ‚úÖ **Loaded the dataset** from iris_dataset.csv\n",
    "3. ‚úÖ **Performed initial inspection** of the data structure\n",
    "4. ‚úÖ **Checked for missing values** (None found)\n",
    "5. ‚úÖ **Identified and removed duplicate rows** (if any)\n",
    "6. ‚úÖ **Validated data types** (all correct)\n",
    "7. ‚úÖ **Checked for outliers** (natural variations, not removed)\n",
    "8. ‚úÖ **Standardized class names** (already consistent)\n",
    "9. ‚úÖ **Saved the cleaned dataset** as iris_cleaned.csv\n",
    "\n",
    "## üéØ Key Achievements:\n",
    "- **Data Quality**: Ensured high-quality, clean dataset\n",
    "- **Consistency**: All data types and formats are consistent\n",
    "- **Reliability**: Removed duplicates and handled potential issues\n",
    "- **Readiness**: Dataset is now ready for visualization and modeling\n",
    "\n",
    "## üöÄ Next Steps:\n",
    "The cleaned dataset is now ready for:\n",
    "- **Task 3**: Data Visualization\n",
    "- **Task 4**: Machine Learning Modeling\n",
    "\n",
    "**Data Cleaning Process Complete!** üéâ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
